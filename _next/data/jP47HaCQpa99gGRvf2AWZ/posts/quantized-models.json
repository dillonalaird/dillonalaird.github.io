{"pageProps":{"postData":{"id":"quantized-models","contentHtml":"<h2>Introduction</h2>\n<p>Several papers have come out recently showing how to run large language models with much less memory\nso they can be and infer on smaller devices such as <a href=\"https://arxiv.org/abs/2208.07339\">LLM.int8()</a> and\n<a href=\"https://arxiv.org/abs/2305.14314\">QLoRA</a>. I wanted to better understand how they work and also apply\nthem to <a href=\"https://arxiv.org/abs/2010.11929\">transformer vision models</a>. In this blog post I'll go\nthrough the different quantization schemes presented in <a href=\"https://arxiv.org/abs/2208.07339\">LLM.int8()</a>\nalong with simple numpy implementations and at the end I also show how to use int8 and 4-bit\nquantization on the <a href=\"https://github.com/facebookresearch/segment-anything/tree/main\">Segment-Anything-Model (SAM)</a>\nbackbone to improve the memory consumption. If you haven't already I also highly recommend checking\nout Tim Dettmer's <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\">blog on 8-bit matrix multiplication</a>\nand <a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\">4-bit quantization</a> which served as\nthe basis for writing this blog.</p>\n<h2>Quantization</h2>\n<p>Quantization in machine learning is a way to reduce latency and memory by representing numbers with\nfewer bits. I'm going to assume you are familiar with how floating point numbers are represented and\nget right into int8 quantization techniques. The main objective is to figure out how best to quantize\nnumbers such that you loose the least amount of information. We'll cover three ways to do this, the first\nbeing <strong>absmax quantization</strong>.</p>\n<h3>Absmax</h3>\n<p><strong>Absmax quantization</strong> is fairly simple, you just scale the input into\nthe range [-127, 127] by dividing by the maximum value and then multiplying by 127. Here's a small\npython script to compute it:\n<img src=\"/images/absmax_quantize.png\" alt=\"\">\nHere <code>s_x</code> si the scaling factor, so the 127 / max(|X|), which we multiply by X to move it to the range\nof -127 to 127. For example:\n<img src=\"/images/absmax_quantize_example.png\" alt=\"\">\nYou can see above the largest number, 4, went to 127 and the scaling factor used was 31.75 where 4 * 31.75\n= 127. You can dequantize this with the following python code:\n<img src=\"/images/absmax_dequantize.png\" alt=\"\">\nDequantizing the above array we get:\n<img src=\"/images/absmax_dequantize_example.png\" alt=\"\">\nBut what happens if we only have positive values? We end up only using half of the quantization range:\n<img src=\"/images/absmax_quantize_bad_example.png\" alt=\"\">\nThis can lead to quantization errors. This leads us to <strong>zeropoint quantization</strong>.</p>\n<h3>Zeropoint Quantization</h3>\n<p><strong>Zeropoint quantization</strong> solves this issue by scaling then shifting the numbers. First we scale the\ninput by the normalized dynamic range <code>nd</code>:\n<img src=\"/images/dyna_example.png\" alt=\"\">\nThen we shift by the zeropoint <code>zp</code>\n<img src=\"/images/zeropoint_example.png\" alt=\"\">\nSo we are rescaling to a new range, the size of which is 255:\n<img src=\"/images/scale.png\" alt=\"\">\nAnd then moving the minimum value into this new range, offsetting it by 128, which is half the size\nof our new range, and using that to shift the range over 0:\n<img src=\"/images/offset.png\" alt=\"\">\nPutting it all together in python with a few other checks we have:\n<img src=\"/images/zeropoint_quantize.png\" alt=\"\">\nAnd to dequantize we simply subtract the zero point and divide by the scale:\n<img src=\"/images/zeropoint_dequantize.png\" alt=\"\">\nTo get even better quantization results, you can also apply either <strong>absmax</strong> or <strong>zeropoint</strong> quantization\nper row or column of a matrix. This helps deal with more variability in the input. You can find a good\noverview of zeropoint quantization (also called affine quantization) <a href=\"https://arxiv.org/abs/2004.09602\">here</a>\nin secction 3.1.1. It turns out this still isn't enough to get quantization to work well for larger models\nas important outlier features can lead to quantization errors.</p>\n<h3>LLM.int8() Quantization</h3>\n<p>Tim Dettmers was able to solve this issue in his <a href=\"https://arxiv.org/abs/2208.07339\"><strong>LLM.int8()</strong> paper</a>\nby introducing a hybrid approach. In the paper he notes that outlier features with large magnitudes start\nto emerge with larger transformers and have a strong affect on attention and prediction performance. To\npreserve these features we simply extract outlier features from the input X and multiply those in float16\nwhile we quantize the rest to int8. Here we assume outlier features have magnitude 2 or more:\n<img src=\"/images/llm.int8_outliers.png\" alt=\"\"></p>\n<p>Once you have to two separate sets of matrices, you can use whatever int8 implementation you want,\n<strong>absmax</strong> or <strong>zeropoint</strong>, the outlier matrices will be multiplied in fp16 and both results added\ntogether:\n<img src=\"/images/llm.int8_mult.png\" alt=\"\">\nWhere <code>row_wise</code> and <code>col_wise</code> quantize functions can be either <strong>absmax</strong> or <strong>zeropoint</strong> but applied\nper row or per column as described above. I also recommend checking out Tim Dettmers's\n<a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\">blog</a> which has a great animation of the above\ncomputation. You can find all the code for the above implementations <a href=\"https://github.com/dillonalaird/quantized-sam\">here</a></p>\n<h3>Using INT8/NF4 For Vision Models</h3>\n<p>While the <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\">blog</a> shows how easy it is to apply\nint8 quantization to language models from huggingface, I found it difficult to apply it to other models\nsuch as the SAM backbone. To run the following code you'll first want to install bitsandbytes following\nthe instructions from their <a href=\"https://github.com/TimDettmers/bitsandbytes\">github page</a>. Then make sure\nyou install the latest versions of these libraries from github</p>\n<pre><code>pip install --upgrade git+https://github.com/huggingface/transformers.git\npip install --upgrade git+https://github.com/huggingface/accelerate.git\n</code></pre>\n<p>You'll need to build the model in fp16 as the int8 module only works with fp16 input for now. Then\ncall <code>replace_with_bnb_layer</code> which will replace all linear layers with the 8 bit linear layer\n(or 4 bit if you choose to use that layer). You can see before calling, we have typical <code>Linear</code>\nlayers:\n<img src=\"/images/sam_model.png\" alt=\"\">\nand after calling they turn into <code>Linear8BitLt</code> layers:\n<img src=\"/images/sam_model_int8.png\" alt=\"\"></p>\n<p>If we print out one of the layers <code>model.blacks[0].attn.qkv</code> it prints\n<code>Parameter(Int8Params(..., device='meta', size=(3840, 1280)))</code> which is the int8 parameters. But\nit's still emtpy and set to the <code>meta</code> device. To fill in the weights we need to call\n<code>set_module_quantized_tensor_to_device</code>, which now allows us to see the quantized weights by\nprinting the layer again:\n<img src=\"/images/sam_model_int8_params.png\" alt=\"\">\nFinally we can call the model by passing it a half precision input, all the steps together look\nlike this:\n<img src=\"/images/bnb_quantize_example.png\" alt=\"\">\nTo get this working with the SAM model you must insure the <code>Linear</code> layers it replaces are doing\nmatrix multiplication on 2d matrices. I've done this in the repository\n<a href=\"https://github.com/dillonalaird/quantized-sam\">here</a> as well as added all the quantization\nfunctions above so you can play around with it. Below are some latency and memory numbers on an\nRTX A5000 (you can find the 4bit conversion in the repository code):\n<img src=\"/images/table.png\" alt=\"\">\nYou can see the max allocated memory reduces by about ~1GB from 32bit to 8bit and you get a\nlittle half the latency. The latency times are about ~1.5x slower on 8bit than 16bit but the\nauthors are working to decrease that time. The relative decrease in max allocated memory is\nnot that much compared to 32 or 16 bit but if we calculate the actual model size we get a more\nclear picture:\n<img src=\"/images/model_sizes.png\" alt=\"\">\nThis shows about a 75% reduction from 32bit to 8bit and then halving it again when we go to\n4 bit!</p>\n<h3>Conclusion</h3>\n<p>To recap we've covered two basic types of quantization, <strong>absmax</strong> and <strong>zeropoint</strong> quantization.\nWe've also shown how to quantize a vision transformer model, SAM, using the bitsandbytes library\ngiving us up to an 86% reduction in model size! All the quantization example code as well as code\nfor quantizing and running the SAM backbone are available <a href=\"https://github.com/dillonalaird/quantized-sam\">here</a></p>\n","title":"How Quantization Works & Quantizing SAM","date":"2023-06-20","image":"/images/scale.png"}},"__N_SSG":true}