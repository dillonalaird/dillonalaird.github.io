{"pageProps":{"postData":{"id":"peculiarities-of-nns","contentHtml":"<p>Neural networks are notoriously difficult to train. They can completely diverge\non one set of parameters and get state-of-the-art results on a slightly\ndifferent set of parameters. Over the years I’ve learned several tricks to\ntraining neural networks. A lot of these tricks are well kept secrets in\nindustry and academics and I think it’s time the rest of the world learned how\nneural networks are really trained. So at the risk of being excommunicated from\nthe deep learning community, here is my write up of the peculiarities of\ntraining neural networks.</p>\n<p>While the advancement of artificial intelligence (AI) has led neural networks\ncloser to obtaining human intelligence, not all networks have moved towards this\npath in a straight line. Some networks have also attained more undesirable human\nattributes. These are often little discussed and even brushed under the rug for\nfear of embarrassment to the deep learning community. But here, I finally\npresent a formal discussion of some of these abnormalities of training neural\nnetworks.</p>\n<h2>Cat Images</h2>\n<p><img src=\"/images/cat.png\" alt=\"\"></p>\n<p>It’s been well known in the deep learning community for a long time that\ntraining neural networks on cat images actually improves performance. Like many\nother oddities of neural networks, researchers have absolutely no clue why this\nis the case but has been well documented in many amateur papers posted on arxiv.\nSome have attributed to happier gradient flows, which have an easier time\nupdating the weights than say melancholy gradient flows, or gradient flows that\nhaven’t had their morning coffee. In some instances, researchers have posted\naccuracy gains up to 2% on ImageNet. In more recent work, researchers have also\nfound that feeding the network pictures of Donald Trump led to a whopping 5%\ndecrease in accuracy. When researchers investigated further, they found that the\nnetwork predicted “baboon” about 5% of the time.</p>\n<h2>\"Humbling\" Your Network</h2>\n<p>Some of the more recent networks achieving state-of-the-art results have become\nvery large and require a lot of hardware to train them. This has led to these\nnetworks developing a slight ego which leads to many issues during test time.\nFor example Google’s GNMT is known to be an arrogant network because it requires\n$500,000 of equipment to train. You might ask it to translate “The duck says\nquack.” to German, to which it either outputs “...” or “Really? You want me to\ntranslate that?”. One way to fix this is by humbling your network. You can do\nthis by feeding it sentences such as, “You’re not that good.” or “You could get\nreplaced by a linear model and no one would know the difference.” or even “I\ncould not anneal the learning rate and watch you diverge at any minute.”. For\nmost people though arrogant networks are not an issue, since only Google runs\ntheir models on half a million dollars of equipment for a month to get\nstate-of-the-art.</p>\n<h2>Ritual Sacrifice</h2>\n<p><img src=\"/images/nvidia.png\" alt=\"\"></p>\n<p>Ever wonder how deep learning researchers find those obscure hyperparameters? In\nschool they’ll tell you it’s random searching but there’s a much darker secret\nbehind them. One way to accomplish this is to sacrifice a GPU before you run\nyour random hyperparameter search. You can do this by creating a fire sigil on\nthe ground in the shape of the Nvidia logo and then burning a GPU in the middle\nof it. The better the GPU, the better the hyperparameters. It’s rumored that\nGoogle sacrificed 3 Titan X’s to obtain their hyperparameters for “Google’s\nNeural Machine Translation System” paper. Since a typical run takes 6 days to\ntrain on 96 NVIDIA K80 GPUs these would have been near impossible to find\notherwise.</p>\n<h2>Home Field Advantage</h2>\n<p>Another well kept secret at Google is that the closer Jeff Dean is to your GPU\ncluster, the faster it runs. This is the real reason Google started developing\nthe TPU, as an insurance policy for when Jeff Dean leaves. Other large companies\nalso have their own in house advantage. Yann Lecun gives +3 BLEU score to French\ntranslation models when sitting near the GPUs.</p>\n","title":"The Peculiarities of Training Neural Networks","date":"2017-07-04"}},"__N_SSG":true}